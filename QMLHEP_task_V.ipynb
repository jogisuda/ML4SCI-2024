{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Lie-Equivariant GNN (Q-LieEGNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### in this task, we were asked to implement and draw the architecture of a possible Quantum Graph Neural Network\n",
    "\n",
    "I will start from the LorentzNet paper and official implementation, load the quark-gluon tagging data, and perform equivariance tests just for sanity checks, to confirm that the code works.\n",
    "\n",
    "Once this is done, here I put a simple modification to incorporate parameterized circuits using Pennylane. Once this is done, we test again for equivariance. Once equivariance test is passed, I show equivariance to an arbitrary metric tensor $J$, where the Lorentz boosts are now symmetry-breaking, but rotations about a fixed plane are preserving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in /home/jogi/anaconda3/lib/python3.9/site-packages (2.5.2)\n",
      "Requirement already satisfied: tqdm in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (4.65.0)\n",
      "Requirement already satisfied: numpy in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (1.26.3)\n",
      "Requirement already satisfied: scipy in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (1.11.4)\n",
      "Requirement already satisfied: fsspec in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: aiohttp in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (3.9.3)\n",
      "Requirement already satisfied: requests in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (2.31.0)\n",
      "Requirement already satisfied: pyparsing in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (3.1.1)\n",
      "Requirement already satisfied: scikit-learn in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (1.4.1.post1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (5.9.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jogi/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jogi/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jogi/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from jinja2->torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jogi/anaconda3/lib/python3.9/site-packages (from requests->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jogi/anaconda3/lib/python3.9/site-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jogi/anaconda3/lib/python3.9/site-packages (from requests->torch_geometric) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jogi/anaconda3/lib/python3.9/site-packages (from requests->torch_geometric) (2023.11.17)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch_geometric) (3.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting torch_sparse\n",
      "  Using cached torch_sparse-0.6.18.tar.gz (209 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-lcy59mga/torch-sparse_ff3c8ac0008f4b1d8e47a3a9bea01e6f/setup.py\", line 8, in <module>\n",
      "  \u001b[31m   \u001b[0m     import torch\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'torch'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[?25hCollecting torch_scatter\n",
      "  Using cached torch_scatter-2.1.2.tar.gz (108 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ch0ak2wl/torch-scatter_6bea335503cc468b8f0c565fd8280640/setup.py\", line 8, in <module>\n",
      "  \u001b[31m   \u001b[0m     import torch\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'torch'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# For Colab\n",
    "!pip install torch_geometric\n",
    "!pip install torch_sparse\n",
    "!pip install torch_scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pennylane as qml\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "\n",
    "dev = qml.device('default.qubit', wires=n_qubits)\n",
    "\n",
    "\n",
    "def H_layer(nqubits):\n",
    "    \"\"\"Layer of single-qubit Hadamard gates.\n",
    "    \"\"\"\n",
    "    for idx in range(nqubits):\n",
    "        qml.Hadamard(wires=idx)\n",
    "\n",
    "\n",
    "def RY_layer(w):\n",
    "    \"\"\"Layer of parametrized qubit rotations around the y axis.\n",
    "    \"\"\"\n",
    "    for idx, element in enumerate(w):\n",
    "        qml.RY(element, wires=idx)\n",
    "\n",
    "\n",
    "def entangling_layer(nqubits):\n",
    "    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n",
    "    \"\"\"\n",
    "    # In other words it should apply something like :\n",
    "    # CNOT  CNOT  CNOT  CNOT...  CNOT\n",
    "    #   CNOT  CNOT  CNOT...  CNOT\n",
    "    for i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    for i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_net(q_input_features, q_weights_flat, q_depth, n_qubits):\n",
    "    \"\"\"\n",
    "    The variational quantum circuit.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape weights\n",
    "    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n",
    "\n",
    "    # Start from state |+> , unbiased w.r.t. |0> and |1>\n",
    "    H_layer(n_qubits)\n",
    "\n",
    "    # Embed features in the quantum node\n",
    "    RY_layer(q_input_features)\n",
    "\n",
    "    # Sequence of trainable variational layers\n",
    "    for k in range(q_depth):\n",
    "        entangling_layer(n_qubits)\n",
    "        RY_layer(q_weights[k])\n",
    "\n",
    "    # Expectation values in the Z basis\n",
    "    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n",
    "    return tuple(exp_vals)\n",
    "\n",
    "\n",
    "class DressedQuantumNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Torch module implementing the *dressed* quantum net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_qubits, q_depth = 1, q_delta=0.001):\n",
    "        \"\"\"\n",
    "        Definition of the *dressed* layout.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.q_depth = q_depth\n",
    "        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        \"\"\"\n",
    "        Defining how tensors are supposed to move through the *dressed* quantum\n",
    "        net.\n",
    "        \"\"\"\n",
    "\n",
    "        # Quantum Embedding (U(X))\n",
    "        q_in = torch.tanh(input_features) * np.pi / 2.0\n",
    "\n",
    "        # Apply the quantum circuit to each element of the batch and append to q_out\n",
    "        q_out = torch.Tensor(0, self.n_qubits)\n",
    "        q_out = q_out.to(device)\n",
    "        # for batch in q_in:\n",
    "        for elem in q_in:\n",
    "            q_out_elem = quantum_net(elem, self.q_params, self.q_depth, self.n_qubits).float().unsqueeze(0)\n",
    "            q_out = torch.cat((q_out, q_out_elem))\n",
    "\n",
    "        # return the batch measurement of the PQC\n",
    "        return q_out.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N nodes: N nodes:   N nodes: N nodes: 139139 \n",
      "\n",
      " 139\n",
      "139\n",
      "N nodes:  N nodes: 139 N nodes: 139\n",
      " 139\n",
      "N nodes: \n",
      " 139\n",
      "N nodes:  139\n",
      "torch.Size([1]) torch.Size([1, 139, 4]) torch.Size([1, 139, 8]) torch.Size([1, 139]) torch.Size([1, 139, 139]) torch.Size([306]) torch.Size([306])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import energyflow\n",
    "from scipy.sparse import coo_matrix\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "def get_adj_matrix(n_nodes, batch_size, edge_mask):\n",
    "    rows, cols = [], []\n",
    "    for batch_idx in range(batch_size):\n",
    "        nn = batch_idx*n_nodes\n",
    "        x = coo_matrix(edge_mask[batch_idx])\n",
    "        rows.append(nn + x.row)\n",
    "        cols.append(nn + x.col)\n",
    "    rows = np.concatenate(rows)\n",
    "    cols = np.concatenate(cols)\n",
    "\n",
    "    edges = [torch.LongTensor(rows), torch.LongTensor(cols)]\n",
    "    return edges\n",
    "\n",
    "def collate_fn(data):\n",
    "    data = list(zip(*data)) # label p4s nodes atom_mask\n",
    "    data = [torch.stack(item) for item in data]\n",
    "    batch_size, n_nodes, _ = data[1].size()\n",
    "    atom_mask = data[-1]\n",
    "    edge_mask = atom_mask.unsqueeze(1) * atom_mask.unsqueeze(2)\n",
    "    diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n",
    "    edge_mask *= diag_mask\n",
    "    edges = get_adj_matrix(n_nodes, batch_size, edge_mask)\n",
    "    return data + [edge_mask, edges]\n",
    "\n",
    "def retrieve_dataloaders(batch_size, num_data = -1, use_one_hot = True, cache_dir = './data', num_workers=4):\n",
    "    raw = energyflow.qg_jets.load(num_data=num_data, pad=True, ncol=4, generator='pythia',\n",
    "                            with_bc=False, cache_dir=cache_dir)\n",
    "    splits = ['train', 'val', 'test']\n",
    "    data = {type:{'raw':None,'label':None} for type in splits}\n",
    "    (data['train']['raw'],  data['val']['raw'],   data['test']['raw'],\n",
    "    data['train']['label'], data['val']['label'], data['test']['label']) = \\\n",
    "        energyflow.utils.data_split(*raw, train=0.8, val=0.1, test=0.1, shuffle = False)\n",
    "\n",
    "    enc = OneHotEncoder(handle_unknown='ignore').fit([[11],[13],[22],[130],[211],[321],[2112],[2212]])\n",
    "    \n",
    "    for split, value in data.items():\n",
    "        pid = torch.from_numpy(np.abs(np.asarray(value['raw'][...,3], dtype=int))).unsqueeze(-1)\n",
    "        p4s = torch.from_numpy(energyflow.p4s_from_ptyphipids(value['raw'],error_on_unknown=True))\n",
    "        one_hot = enc.transform(pid.reshape(-1,1)).toarray().reshape(pid.shape[:2]+(-1,))\n",
    "        one_hot = torch.from_numpy(one_hot)\n",
    "        mass = torch.from_numpy(energyflow.ms_from_p4s(p4s)).unsqueeze(-1)\n",
    "        charge = torch.from_numpy(energyflow.pids2chrgs(pid))\n",
    "        if use_one_hot:\n",
    "            nodes = one_hot\n",
    "        else:\n",
    "            nodes = torch.cat((mass,charge),dim=-1)\n",
    "            nodes = torch.sign(nodes) * torch.log(torch.abs(nodes) + 1)\n",
    "        atom_mask = (pid[...,0] != 0)\n",
    "        value['p4s'] = p4s\n",
    "        value['nodes'] = nodes\n",
    "        value['label'] = torch.from_numpy(value['label'])\n",
    "        value['atom_mask'] = atom_mask.to(torch.bool)\n",
    "\n",
    "    datasets = {split: TensorDataset(value['label'], value['p4s'],\n",
    "                                     value['nodes'], value['atom_mask'])\n",
    "                for split, value in data.items()}\n",
    "\n",
    "    # distributed training\n",
    "    # train_sampler = DistributedSampler(datasets['train'], shuffle=True)\n",
    "    # Construct PyTorch dataloaders from datasets\n",
    "    dataloaders = {split: DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     # sampler=train_sampler if (split == 'train') else DistributedSampler(dataset, shuffle=False),\n",
    "                                     pin_memory=False,\n",
    "                                     # persistent_workers=True,\n",
    "                                     drop_last=True if (split == 'train') else False,\n",
    "                                     num_workers=num_workers,\n",
    "                                     collate_fn=collate_fn)\n",
    "                        for split, dataset in datasets.items()}\n",
    "\n",
    "    return dataloaders #train_sampler, dataloaders\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # train_sampler, dataloaders = retrieve_dataloaders(32, 100)\n",
    "    dataloaders = retrieve_dataloaders(1, 20)\n",
    "    for (label, p4s, nodes, atom_mask, edge_mask, edges) in dataloaders['train']:\n",
    "        print(label.shape, p4s.shape, nodes.shape, atom_mask.shape,\n",
    "              edge_mask.shape, edges[0].shape, edges[1].shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "n_nodes = 139\n",
    "device = 'cpu'\n",
    "dtype = torch.float32\n",
    "\n",
    "atom_positions = p4s[:, :, :].view(batch_size * n_nodes, -1).to(device, dtype)\n",
    "\n",
    "atom_mask = atom_mask.view(batch_size * n_nodes, -1).to(device, dtype)\n",
    "edge_mask = edge_mask.reshape(batch_size * n_nodes * n_nodes, -1).to(device)\n",
    "\n",
    "edges = [a.to(device) for a in edges]\n",
    "nodes = nodes.view(batch_size * n_nodes, -1).to(device,dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "class LGEB(nn.Module):\n",
    "    def __init__(self, n_input, n_output, n_hidden, n_node_attr=0,\n",
    "                 dropout = 0., c_weight=1.0, last_layer=False, A=None, include_x=False):\n",
    "        super(LGEB, self).__init__()\n",
    "        self.c_weight = c_weight\n",
    "        n_edge_attr = 2 if not include_x else 10 # dims for Minkowski norm & inner product\n",
    "\n",
    "        self.include_x = include_x\n",
    "        self.phi_e = nn.Sequential(\n",
    "            nn.Linear(n_input * 2 + n_edge_attr, n_hidden, bias=False),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.phi_h = nn.Sequential(\n",
    "            nn.Linear(n_hidden + n_input + n_node_attr, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_output))\n",
    "\n",
    "        layer = nn.Linear(n_hidden, 1, bias=False)\n",
    "        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n",
    "\n",
    "        self.phi_x = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            layer)\n",
    "\n",
    "        self.phi_m = nn.Sequential(\n",
    "            nn.Linear(n_hidden, 1),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        self.last_layer = last_layer\n",
    "        if last_layer:\n",
    "            del self.phi_x\n",
    "\n",
    "        self.A = A\n",
    "        self.norm_fn = normA_fn(A) if A is not None else normsq4\n",
    "        self.dot_fn = dotA_fn(A) if A is not None else dotsq4\n",
    "        \n",
    "\n",
    "    def m_model(self, hi, hj, norms, dots):\n",
    "        out = torch.cat([hi, hj, norms, dots], dim=1)\n",
    "        out = self.phi_e(out)\n",
    "        # print(\"m_model output: \", out.shape)\n",
    "        w = self.phi_m(out)\n",
    "        out = out * w\n",
    "        return out\n",
    "\n",
    "    def m_model_extended(self, hi, hj, norms, dots, xi, xj):\n",
    "        out = torch.cat([hi, hj, norms, dots, xi, xj], dim=1)\n",
    "        out = self.phi_e(out)\n",
    "        w = self.phi_m(out)\n",
    "        out = out * w\n",
    "        return out\n",
    "\n",
    "    def h_model(self, h, edges, m, node_attr):\n",
    "        i, j = edges\n",
    "        agg = unsorted_segment_sum(m, i, num_segments=h.size(0))\n",
    "        agg = torch.cat([h, agg, node_attr], dim=1)\n",
    "        out = h + self.phi_h(agg)\n",
    "        return out\n",
    "\n",
    "    def x_model(self, x, edges, x_diff, m): # norms\n",
    "        i, j = edges\n",
    "        trans = x_diff * self.phi_x(m)\n",
    "        # print(\"m: \", m.shape)\n",
    "        # print(\"trans: \", trans.shape)\n",
    "        # From https://github.com/vgsatorras/egnn\n",
    "        # This is never activated but just in case it explosed it may save the train\n",
    "        trans = torch.clamp(trans, min=-100, max=100)\n",
    "        # print(\"trans: \", trans.shape)\n",
    "        # print(\"x.size: \", x.size(0))\n",
    "        agg = unsorted_segment_mean(trans, i, num_segments=x.size(0))\n",
    "        x = x + agg * self.c_weight # * norms[i, j], smth like that, or norms\n",
    "        return x\n",
    "\n",
    "    def minkowski_feats(self, edges, x):\n",
    "        i, j = edges\n",
    "        x_diff = x[i] - x[j]\n",
    "        norms = self.norm_fn(x_diff).unsqueeze(1)\n",
    "        dots = self.dot_fn(x[i], x[j]).unsqueeze(1)\n",
    "        norms, dots = psi(norms), psi(dots)\n",
    "        return norms, dots, x_diff\n",
    "\n",
    "    def forward(self, h, x, edges, node_attr=None):\n",
    "        i, j = edges\n",
    "        norms, dots, x_diff = self.minkowski_feats(edges, x)\n",
    "\n",
    "        if self.include_x:\n",
    "            m = self.m_model_extended(h[i], h[j], norms, dots, x[i], x[j])\n",
    "        else:\n",
    "            m = self.m_model(h[i], h[j], norms, dots) # [B*N, hidden]\n",
    "        if not self.last_layer:\n",
    "            # print(\"X: \", x)\n",
    "            x = self.x_model(x, edges, x_diff, m)\n",
    "            # print(\"phi_x(X) = \", x, '\\n---\\n')\n",
    "            \n",
    "        h = self.h_model(h, edges, m, node_attr)\n",
    "        return h, x, m\n",
    "\n",
    "class LorentzNet(nn.Module):\n",
    "    r''' Implementation of LorentzNet.\n",
    "\n",
    "    Args:\n",
    "        - `n_scalar` (int): number of input scalars.\n",
    "        - `n_hidden` (int): dimension of latent space.\n",
    "        - `n_class`  (int): number of output classes.\n",
    "        - `n_layers` (int): number of LGEB layers.\n",
    "        - `c_weight` (float): weight c in the x_model.\n",
    "        - `dropout`  (float): dropout rate.\n",
    "    '''\n",
    "    def __init__(self, n_scalar, n_hidden, n_class = 2, n_layers = 6, c_weight = 1e-3, dropout = 0., A=None, include_x=False):\n",
    "        super(LorentzNet, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Linear(n_scalar, n_hidden)\n",
    "        self.LGEBs = nn.ModuleList([LGEB(self.n_hidden, self.n_hidden, self.n_hidden, \n",
    "                                    n_node_attr=n_scalar, dropout=dropout,\n",
    "                                    c_weight=c_weight, last_layer=(i==n_layers-1), A=A, include_x=include_x)\n",
    "                                    for i in range(n_layers)])\n",
    "        self.graph_dec = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Dropout(dropout),\n",
    "                                       nn.Linear(self.n_hidden, n_class)) # classification\n",
    "\n",
    "    def forward(self, scalars, x, edges, node_mask, edge_mask, n_nodes):\n",
    "        h = self.embedding(scalars)\n",
    "\n",
    "        print(\"h before (just the first particle): \\n\", h[0].cpu().detach().numpy())\n",
    "        for i in range(self.n_layers):\n",
    "            h, x, _ = self.LGEBs[i](h, x, edges, node_attr=scalars)\n",
    "        print(\"h after (just the first particle): \\n\", h[0].cpu().detach().numpy())\n",
    "            \n",
    "        h = h * node_mask\n",
    "        h = h.view(-1, n_nodes, self.n_hidden)\n",
    "        h = torch.mean(h, dim=1)\n",
    "        pred = self.graph_dec(h)\n",
    "\n",
    "        print(\"Final preds: \\n\", pred.cpu().detach().numpy())\n",
    "        return pred.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have the official code for the classical, just for sanity checking, let's test for equivariance\n",
    "\n",
    "The cell below is just an auxiliary function to give us the boosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "# Speed of light (m/s)\n",
    "c = 299792458\n",
    "\n",
    "\"\"\"Lorentz transformations describe the transition between two inertial reference\n",
    "frames F and F', each of which is moving in some direction with respect to the\n",
    "other. This code only calculates Lorentz transformations for movement in the x\n",
    "direction with no spatial rotation (i.e., a Lorentz boost in the x direction).\n",
    "The Lorentz transformations are calculated here as linear transformations of\n",
    "four-vectors [ct, x, y, z] described by Minkowski space. Note that t (time) is\n",
    "multiplied by c (the speed of light) in the first entry of each four-vector.\n",
    "\n",
    "Thus, if X = [ct; x; y; z] and X' = [ct'; x'; y'; z'] are the four-vectors for\n",
    "two inertial reference frames and X' moves in the x direction with velocity v\n",
    "with respect to X, then the Lorentz transformation from X to X' is X' = BX,\n",
    "where\n",
    "\n",
    "    | γ  -γβ  0  0|\n",
    "B = |-γβ  γ   0  0|\n",
    "    | 0   0   1  0|\n",
    "    | 0   0   0  1|\n",
    "\n",
    "is the matrix describing the Lorentz boost between X and X',\n",
    "γ = 1 / √(1 - v²/c²) is the Lorentz factor, and β = v/c is the velocity as\n",
    "a fraction of c.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def beta(velocity: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculates β = v/c, the given velocity as a fraction of c\n",
    "    >>> beta(c)\n",
    "    1.0\n",
    "    >>> beta(199792458)\n",
    "    0.666435904801848\n",
    "    \"\"\"\n",
    "    if velocity > c:\n",
    "        raise ValueError(\"Speed must not exceed light speed 299,792,458 [m/s]!\")\n",
    "    elif velocity < 1:\n",
    "        # Usually the speed should be much higher than 1 (c order of magnitude)\n",
    "        raise ValueError(\"Speed must be greater than or equal to 1!\")\n",
    "\n",
    "    return velocity / c\n",
    "\n",
    "\n",
    "def gamma(velocity: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Lorentz factor γ = 1 / √(1 - v²/c²) for a given velocity\n",
    "    >>> gamma(4)\n",
    "    1.0000000000000002\n",
    "    >>> gamma(1e5)\n",
    "    1.0000000556325075\n",
    "    >>> gamma(3e7)\n",
    "    1.005044845777813\n",
    "    >>> gamma(2.8e8)\n",
    "    2.7985595722318277\n",
    "    \"\"\"\n",
    "    return 1 / sqrt(1 - beta(velocity) ** 2)\n",
    "\n",
    "\n",
    "def transformation_matrix(velocity: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the Lorentz transformation matrix for movement in the x direction:\n",
    "\n",
    "    | γ  -γβ  0  0|\n",
    "    |-γβ  γ   0  0|\n",
    "    | 0   0   1  0|\n",
    "    | 0   0   0  1|\n",
    "\n",
    "    where γ is the Lorentz factor and β is the velocity as a fraction of c\n",
    "    >>> transformation_matrix(29979245)\n",
    "    array([[ 1.00503781, -0.10050378,  0.        ,  0.        ],\n",
    "           [-0.10050378,  1.00503781,  0.        ,  0.        ],\n",
    "           [ 0.        ,  0.        ,  1.        ,  0.        ],\n",
    "           [ 0.        ,  0.        ,  0.        ,  1.        ]])\n",
    "    \"\"\"\n",
    "    return np.array(\n",
    "        [\n",
    "            [gamma(velocity), -gamma(velocity) * beta(velocity), 0, 0],\n",
    "            [-gamma(velocity) * beta(velocity), gamma(velocity), 0, 0],\n",
    "            [0, 0, 1, 0],\n",
    "            [0, 0, 0, 1],\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LorentzNet(n_scalar = 8, n_hidden = 4, n_class = 2,\\\n",
    "                       dropout = 0.2, n_layers = 6,\\\n",
    "                       c_weight = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with a default prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [ 0.22275785 -0.00887579 -0.45730796  0.4752541 ]\n",
      "h after (just the first particle): \n",
      " [ 0.6250086  -0.4157089   0.19434586  3.7227166 ]\n",
      "Final preds: \n",
      " [[0.25635535 0.08354717]]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... taking any random nonsense transformation in the four-momentum vectors\n",
    "i.e.: multiplying by 0.1. Does the hidden rep stay the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [ 0.22275785 -0.00887579 -0.45730796  0.4752541 ]\n",
      "h after (just the first particle): \n",
      " [ 1.5326474  -0.09580445 -0.10811514  4.131195  ]\n",
      "Final preds: \n",
      " [[0.25635535 0.08354717]]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x= 0.1 * atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even though the final logits in this case wasn't different, if we look the last output of h (which contains both scalar and 4-momenta information), it changed! Now, what about Lorentz transformations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [ 0.22275785 -0.00887579 -0.45730796  0.4752541 ]\n",
      "h after (just the first particle): \n",
      " [ 0.6251303  -0.41550016  0.1935861   3.721067  ]\n",
      "Final preds: \n",
      " [[0.25635535 0.08354717]]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x= (torch.tensor(transformation_matrix(220000000)) @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivariance works. Now, let's move to our Q-LieEGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "\n",
    "\"\"\"\n",
    "    Quantum Lie-Equivariant Block (QLieGEB).\n",
    "    \n",
    "        - Given the Lie generators found (i.e.: through LieGAN, oracle-preserving latent flow, or some other approach\n",
    "          that we develop further), once the metric tensor J is found via the equation:\n",
    "\n",
    "                          L.J + J.(L^T) = 0,\n",
    "                          \n",
    "          we just have to specify the metric to make the model symmetry-preserving to the corresponding Lie group. \n",
    "          In the cells below, I will show first how the model preserves symmetries (starting with the default Lorentz group),\n",
    "          and when we change J to some other metric (Euclidean, for example), Lorentz boosts break equivariance, while other\n",
    "          transformations preserve it (rotations, for the example shown in the cells below)\n",
    "\"\"\"\n",
    "class QLieGEB(nn.Module):\n",
    "    def __init__(self, n_input, n_output, n_hidden, n_node_attr=0,\n",
    "                 dropout = 0., c_weight=1.0, last_layer=False, A=None, include_x=False):\n",
    "        super(QLieGEB, self).__init__()\n",
    "        self.c_weight = c_weight\n",
    "        n_edge_attr = 2 if not include_x else 10 # dims for Minkowski norm & inner product\n",
    "\n",
    "        self.include_x = include_x\n",
    "\n",
    "        \"\"\"\n",
    "            phi_e: input size: n_qubits -> output size: n_qubits\n",
    "            n_hidden has to be equal to n_input (n_input * 2 + n_edge_attr),\n",
    "            but this is just considering that this is a simple working example.\n",
    "        \"\"\"\n",
    "        self.phi_e = DressedQuantumNet(n_input * 2 + n_edge_attr)\n",
    "\n",
    "        n_hidden = n_input * 2 + n_edge_attr\n",
    "        self.phi_h = nn.Sequential(\n",
    "            nn.Linear(n_hidden + n_input + n_node_attr, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_output))\n",
    "\n",
    "        layer = nn.Linear(n_hidden, 1, bias=False)\n",
    "        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n",
    "\n",
    "        self.phi_x = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            layer)\n",
    "\n",
    "        self.phi_m = nn.Sequential(\n",
    "            nn.Linear(n_hidden, 1),\n",
    "            nn.Sigmoid())        \n",
    "        # self.phi_e = nn.Sequential(\n",
    "        #     nn.Linear(n_input * 2 + n_edge_attr, n_hidden, bias=False),\n",
    "        #     nn.BatchNorm1d(n_hidden),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(n_hidden, n_hidden),\n",
    "        #     nn.ReLU())\n",
    "\n",
    "        # self.phi_h = nn.Sequential(\n",
    "        #     nn.Linear(n_hidden + n_input + n_node_attr, n_hidden),\n",
    "        #     nn.BatchNorm1d(n_hidden),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(n_hidden, n_output))\n",
    "\n",
    "        # layer = nn.Linear(n_hidden, 1, bias=False)\n",
    "        # torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n",
    "\n",
    "        # self.phi_x = nn.Sequential(\n",
    "        #     nn.Linear(n_hidden, n_hidden),\n",
    "        #     nn.ReLU(),\n",
    "        #     layer)\n",
    "\n",
    "        # self.phi_m = nn.Sequential(\n",
    "        #     nn.Linear(n_hidden, 1),\n",
    "        #     nn.Sigmoid())\n",
    "        \n",
    "        self.last_layer = last_layer\n",
    "        if last_layer:\n",
    "            del self.phi_x\n",
    "\n",
    "        self.A = A\n",
    "        self.norm_fn = normA_fn(A) if A is not None else normsq4\n",
    "        self.dot_fn = dotA_fn(A) if A is not None else dotsq4\n",
    "\n",
    "    def m_model(self, hi, hj, norms, dots):\n",
    "        out = torch.cat([hi, hj, norms, dots], dim=1)\n",
    "        # print(\"Before embedding to |psi> : \", out)\n",
    "        out = self.phi_e(out).squeeze(0)\n",
    "        w = self.phi_m(out)\n",
    "        out = out * w\n",
    "        return out\n",
    "\n",
    "    def m_model_extended(self, hi, hj, norms, dots, xi, xj):\n",
    "        out = torch.cat([hi, hj, norms, dots, xi, xj], dim=1)\n",
    "        out = self.phi_e(out).squeeze(0)\n",
    "        w = self.phi_m(out)\n",
    "        out = out * w\n",
    "        return out\n",
    "\n",
    "    def h_model(self, h, edges, m, node_attr):\n",
    "        i, j = edges\n",
    "        agg = unsorted_segment_sum(m, i, num_segments=h.size(0))\n",
    "        agg = torch.cat([h, agg, node_attr], dim=1)\n",
    "        out = h + self.phi_h(agg)\n",
    "        return out\n",
    "\n",
    "    def x_model(self, x, edges, x_diff, m):\n",
    "        i, j = edges\n",
    "        trans = x_diff * self.phi_x(m)\n",
    "        # From https://github.com/vgsatorras/egnn\n",
    "        # This is never activated but just in case it explosed it may save the train\n",
    "        # From https://github.com/vgsatorras/egnn\n",
    "        # This is never activated but just in case it explosed it may save the train\n",
    "        trans = torch.clamp(trans, min=-100, max=100)\n",
    "        agg = unsorted_segment_mean(trans, i, num_segments=x.size(0))\n",
    "        x = x + agg * self.c_weight\n",
    "        return x\n",
    "\n",
    "    def minkowski_feats(self, edges, x):\n",
    "        i, j = edges\n",
    "        x_diff = x[i] - x[j]\n",
    "        norms = self.norm_fn(x_diff).unsqueeze(1)\n",
    "        dots = self.dot_fn(x[i], x[j]).unsqueeze(1)\n",
    "        norms, dots = psi(norms), psi(dots)\n",
    "        return norms, dots, x_diff\n",
    "\n",
    "    def forward(self, h, x, edges, node_attr=None):\n",
    "        i, j = edges\n",
    "        norms, dots, x_diff = self.minkowski_feats(edges, x)\n",
    "\n",
    "        if self.include_x:\n",
    "            m = self.m_model_extended(h[i], h[j], norms, dots, x[i], x[j])\n",
    "        else:\n",
    "            m = self.m_model(h[i], h[j], norms, dots) # [B*N, hidden]\n",
    "        if not self.last_layer:\n",
    "            x = self.x_model(x, edges, x_diff, m)\n",
    "        h = self.h_model(h, edges, m, node_attr)\n",
    "        return h, x, m\n",
    "\n",
    "class QLieEGNN(nn.Module):\n",
    "    r''' Implementation of LorentzNet.\n",
    "\n",
    "    Args:\n",
    "        - `n_scalar` (int): number of input scalars.\n",
    "        - `n_hidden` (int): dimension of latent space.\n",
    "        - `n_class`  (int): number of output classes.\n",
    "        - `n_layers` (int): number of LGEB layers.\n",
    "        - `c_weight` (float): weight c in the x_model.\n",
    "        - `dropout`  (float): dropout rate.\n",
    "    '''\n",
    "    def __init__(self, n_scalar, n_hidden, n_class = 2, n_layers = 6, c_weight = 1e-3, dropout = 0., A=None, include_x=False):\n",
    "        super(QLieEGNN, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Linear(n_scalar, n_hidden)\n",
    "        self.QLieGEBs = nn.ModuleList([QLieGEB(self.n_hidden, self.n_hidden, self.n_hidden, \n",
    "                                    n_node_attr=n_scalar, dropout=dropout,\n",
    "                                    c_weight=c_weight, last_layer=(i==n_layers-1), A=A, include_x=include_x)\n",
    "                                    for i in range(n_layers)])\n",
    "        self.graph_dec = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Dropout(dropout),\n",
    "                                       nn.Linear(self.n_hidden, n_class)) # classification\n",
    "\n",
    "    def forward(self, scalars, x, edges, node_mask, edge_mask, n_nodes):\n",
    "        h = self.embedding(scalars)\n",
    "        \n",
    "        print(\"h before (just the first particle): \\n\", h[0].cpu().detach().numpy())\n",
    "        for i in range(self.n_layers):\n",
    "            h, x, _ = self.QLieGEBs[i](h, x, edges, node_attr=scalars)\n",
    "        \n",
    "        print(\"h after (just the first particle): \\n\", h[0].cpu().detach().numpy())\n",
    "        \n",
    "        h = h * node_mask\n",
    "        h = h.view(-1, n_nodes, self.n_hidden)\n",
    "        h = torch.mean(h, dim=1)\n",
    "        pred = self.graph_dec(h)\n",
    "        return pred.squeeze(1)\n",
    "\n",
    "\n",
    "def unsorted_segment_sum(data, segment_ids, num_segments):\n",
    "    r'''Custom PyTorch op to replicate TensorFlow's `unsorted_segment_sum`.\n",
    "    Adapted from https://github.com/vgsatorras/egnn.\n",
    "    '''\n",
    "    result = data.new_zeros((num_segments, data.size(1)))\n",
    "    result.index_add_(0, segment_ids, data)\n",
    "    return result\n",
    "\n",
    "def unsorted_segment_mean(data, segment_ids, num_segments):\n",
    "    r'''Custom PyTorch op to replicate TensorFlow's `unsorted_segment_mean`.\n",
    "    Adapted from https://github.com/vgsatorras/egnn.\n",
    "    '''\n",
    "    result = data.new_zeros((num_segments, data.size(1)))\n",
    "    count = data.new_zeros((num_segments, data.size(1)))\n",
    "    result.index_add_(0, segment_ids, data)\n",
    "    count.index_add_(0, segment_ids, torch.ones_like(data))\n",
    "    return result / count.clamp(min=1)\n",
    "\n",
    "def normsq4(p):\n",
    "    r''' Minkowski square norm\n",
    "         `\\|p\\|^2 = p[0]^2-p[1]^2-p[2]^2-p[3]^2`\n",
    "    ''' \n",
    "    psq = torch.pow(p, 2)\n",
    "    return 2 * psq[..., 0] - psq.sum(dim=-1)\n",
    "    \n",
    "def dotsq4(p,q):\n",
    "    r''' Minkowski inner product\n",
    "         `<p,q> = p[0]q[0]-p[1]q[1]-p[2]q[2]-p[3]q[3]`\n",
    "    '''\n",
    "    psq = p*q\n",
    "    return 2 * psq[..., 0] - psq.sum(dim=-1)\n",
    "\n",
    "def normA_fn(A):\n",
    "    return lambda p: torch.einsum('...i, ij, ...j->...', p, A, p)\n",
    "\n",
    "def dotA_fn(A):\n",
    "    return lambda p, q: torch.einsum('...i, ij, ...j->...', p, A, q)\n",
    "    \n",
    "def psi(p):\n",
    "    ''' `\\psi(p) = Sgn(p) \\cdot \\log(|p| + 1)`\n",
    "    '''\n",
    "    return torch.sign(p) * torch.log(torch.abs(p) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum model\n",
    "\n",
    "#### Let's start with a default prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QLieEGNN(n_scalar = 8, n_hidden = 4, n_class = 2,\\\n",
    "                       dropout = 0.2, n_layers = 6,\\\n",
    "                       c_weight = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.02530114 -0.01923932 -0.2870935   0.00164617]\n",
      "h after (just the first particle): \n",
      " [ 0.9169079   1.3130671   0.57629734 -0.47118652]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... taking any random nonsense transformation in the four-momentum vectors\n",
    "i.e.: multiplying by 0.1. Does the hidden rep stay the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.02530114 -0.01923932 -0.2870935   0.00164617]\n",
      "h after (just the first particle): \n",
      " [ 2.43436    1.324191  -3.4988296 -1.3810511]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=0.1 * atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not at all! What about Lorentz transformations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.02530114 -0.01923932 -0.2870935   0.00164617]\n",
      "h after (just the first particle): \n",
      " [ 0.91627324  1.3119451   0.5798764  -0.47567284]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=(torch.tensor(transformation_matrix(180000000)) @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivariance holds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's do the predictions again for some other metric tensor J. \n",
    "#### This will illustrate the situation where we found an infinitesimal generator for some experimental data\n",
    "(i.e.: following Robin Walter's approach in LieGAN; the oracle-preserving latents from Roy Forestano et. al, or some other approach that we develop further - would be interesting). Once we have the generators, suppose that we solved for the metric tensor by solving the following eq. (as proposed in Robin's paper):\n",
    "\n",
    "\\begin{equation}\n",
    "L\\cdot J + J\\cdot L^{T} = 0\n",
    "\\end{equation}\n",
    "\n",
    "Here, the Lorentz transformations should not anymore preserve equivariance. To illustrate this, let's consider $J = diag(1,1,1,1)$, that is, we recover the Euclidean norm and dot-product. So, if our model is working, then **boosts** should **break equivariance**, but **rotations** should **preserve** it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J: \n",
      " tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "J = torch.eye(4)\n",
    "print(\"J: \\n\", J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    I will define a rotation matrix about the xy plane. Given that our QLie-EGNN has a new metric,\n",
    "    the Lorentz boosts now should break equivariance, but rotations in this case, should preserve\n",
    "    it.\n",
    "\"\"\"\n",
    "rot = torch.tensor([[np.cos(np.pi), -np.sin(np.pi), 0, 0],\n",
    "                    [np.sin(np.pi), np.cos(np.pi),  0, 0],\n",
    "                    [     0       ,       0      ,  1, 0],\n",
    "                    [     0       ,       0      ,  0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QLieEGNN(n_scalar = 8, n_hidden = 4, n_class = 2,\\\n",
    "                       dropout = 0.2, n_layers = 6,\\\n",
    "                       c_weight = 1e-3, A=J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, the default forward pass using the Euclidean metric J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.20670539  0.26581004 -0.09239267 -0.22357208]\n",
      "h after (just the first particle): \n",
      " [-0.46950197 -3.965734   -3.0378942  -1.9866586 ]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, the Lorentz boosted jets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.20670539  0.26581004 -0.09239267 -0.22357208]\n",
      "h after (just the first particle): \n",
      " [-0.60887957 -3.9931903  -3.0501935  -2.342436  ]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=(torch.tensor(transformation_matrix(240000000)) @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equivariance is broken. What about a rotation about the xy plane?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.20670539  0.26581004 -0.09239267 -0.22357208]\n",
      "h after (just the first particle): \n",
      " [-0.46950197 -3.965734   -3.0378942  -1.9866586 ]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=(rot @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equivariant again.\n",
    "I propose to work on this project, exploring how to improve the symmetry discovery. Also, besides incorporating arbitrary Lie invariances, Infrared Collinear (IRC) safety would be very interesting, and study how our model performs on tagging semi-visible jets for Beyond the Standard Model (BSM) discoveries, like was done in [6] for the Hidden Valley models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "quantum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
