{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ff8584-0c0e-4fa1-8bd9-3c2f6a3b3bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset, Dataset, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "959e9753-fa81-46c1-9aad-905167b08a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/24 14:53:57 WARN Utils: Your hostname, fff002 resolves to a loopback address: 127.0.1.1; using 200.145.157.3 instead (on interface enp5s0)\n",
      "24/03/24 14:53:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/24 14:53:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"40g\").appName(\"ml4sci\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b32fd9-366c-4904-baf4-772f1ba7e27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run0_path = \"datasets/task_ii/jet0_run0.parquet\"\n",
    "run1_path = \"datasets/task_ii/jet0_run1.parquet\"\n",
    "run2_path = \"datasets/task_ii/jet0_run2.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d3c173-9c98-43b5-9f01-c188b9ff27d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_to_tensor(iterator):\n",
    "    for row in iterator:\n",
    "        # Assuming 'array_column' is the name of your multidimensional array column\n",
    "        tensor = torch.tensor(row['X_jets'])\n",
    "        # Now you can save the tensor to disk or do something else with it\n",
    "        yield tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6edc4347-429a-423f-b8fd-0520a3c8263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def save_train_tensor_to_disk(path, tensor, row_index):\n",
    "    path = path + \"X_jet_partition_{}.pt\".format(row_index)\n",
    "    torch.save(tensor, path)\n",
    "\n",
    "# first, mapPartitionWithIndex( f) has to receive some function f that receives the index and iterator for\n",
    "# each partition, and returns (partition_index, iter).\n",
    "# then, forEachPartition( ) receives the iterator for what's inside each partition\n",
    "\n",
    "    \n",
    "# Use foreachPartition to save each partition's tensors to disk\n",
    "# This argument now has both the partition_idx and the tensor iterator!\n",
    "def save_train_partition(iterator):\n",
    "\n",
    "    partition_tensors = []\n",
    "    first_tensor_idx = -1\n",
    "    for tensor, tensor_idx in iterator:\n",
    "        if first_tensor_idx == -1:\n",
    "            first_tensor_idx = tensor_idx\n",
    "            \n",
    "        partition_tensors.append(tensor)\n",
    "\n",
    "    if len(partition_tensors) == 0: #if partition is not empty, then save tensors\n",
    "        return\n",
    "        \n",
    "    partition_tensors = torch.cat(partition_tensors, axis=0)\n",
    "    torch.save(partition_tensors, train_path + \"X_jet_partition_{}.pt\".format(str(first_tensor_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1cebc0cb-e6fd-42c2-8fcc-5b3e77908ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_val_tensor_to_disk(tensor, index):\n",
    "    path = val_path + \"X_jet_partition_{}.pt\".format(index)\n",
    "    torch.save(tensor, path)\n",
    "\n",
    "# Use foreachPartition to save each partition's tensors to disk\n",
    "def save_val_partition(iterator):\n",
    "    # for index, tensor in enumerate(iterator):\n",
    "    #     save_val_tensor_to_disk(tensor, index)\n",
    "    partition_tensors = []\n",
    "    first_tensor_idx = -1\n",
    "    for tensor, tensor_idx in iterator:\n",
    "        if first_tensor_idx == -1:\n",
    "            first_tensor_idx = tensor_idx\n",
    "            \n",
    "        partition_tensors.append(tensor)\n",
    "\n",
    "    if len(partition_tensors) == 0: #if partition is not empty, then save tensors\n",
    "        return\n",
    "        \n",
    "    partition_tensors = torch.cat(partition_tensors, axis=0)\n",
    "    torch.save(partition_tensors, val_path + \"X_jet_partition_{}.pt\".format(str(first_tensor_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b592b2-4a40-423e-a13d-e54b7b9bb113",
   "metadata": {},
   "source": [
    "### Now, we'll load all parquet files via spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8c93259-e0af-4850-a775-3bb03b3e52f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "run0_df = spark.read.parquet(run0_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13464879-b74e-49ce-801c-73c53345f9a1",
   "metadata": {},
   "source": [
    "#### Let's also check the schema and take a peek of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53418603-7aaf-4271-af02-6f69db7b53f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- X_jets: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |-- pt: double (nullable = true)\n",
      " |-- m0: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's check the schema..\n",
    "run0_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80eb05ac-e79a-4bfa-ac19-ec36bbb75458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+---+\n",
      "|              X_jets|                pt|                m0|  y|\n",
      "+--------------------+------------------+------------------+---+\n",
      "|[[[0.0, 0.0, 0.0,...|112.41109466552734|21.098247528076172|0.0|\n",
      "|[[[0.0, 0.0, 0.0,...| 95.22040557861328|14.030599594116211|1.0|\n",
      "|[[[0.0, 0.0, 0.0,...| 97.00731658935547|17.728967666625977|1.0|\n",
      "+--------------------+------------------+------------------+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ..and take a peek:\n",
    "run0_df.show(3, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "892409c6-5c1a-4697-817e-4d2f3d2bccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run1_df = spark.read.parquet(run1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b54ead8-4ec5-4b99-af58-da0e5cc306fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "run2_df = spark.read.parquet(run2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa450d-70f6-42ad-9b6f-759373f7a86e",
   "metadata": {},
   "source": [
    "### Now let's concat all spark dataframes into one single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44211cac-1789-431a-afb9-e45712a61465",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = run0_df.union(run1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62dde088-f21d-4219-ab78-f6a1114c8ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.union(run2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9aaca34-1ee7-45b7-8dac-9a8f3491e0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "139306"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4a2df9ad-3f1b-45dd-a37e-c110cd062456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.68769999999999"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(139306 * 0.180 * 0.2) / 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b786eb-2632-4ea5-ace3-51830816c2b5",
   "metadata": {},
   "source": [
    "### Next, we'll split into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f0419b7-36bc-4396-b997-447e399c0290",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = final_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3deb70-85ea-4984-b890-52a132dec2b7",
   "metadata": {},
   "source": [
    "### Now let's save the tensors locally in our directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6c65d02-a7d7-4d80-84a3-4255480536ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"datasets/task_ii/train/X_jets/\"\n",
    "val_path = \"datasets/task_ii/validation/X_jets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f1c218cb-6750-4efc-b18d-cf25bd3b2c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is important: from the spark DataFrame, we need to create an RDD for efficient data storage.\n",
    "zipWithIndex() will allow us to retrieve, for each jet, not only its tensor, but also its corresponding index.\n",
    "\"\"\"\n",
    "# Apply the function to each partition #zipWithIndex()\n",
    "rdd_train = train_df.rdd.zipWithIndex().map(lambda row: (torch.Tensor(row[0]['X_jets']).expand(1, 3, 125, 125), row[1]) ).repartition(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "db0409b7-4e99-4bc8-96dc-00a2956da318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_train.foreachPartition(save_train_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "158fb0f5-218c-4e51-8186-bb313ad62db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Apply the function to each partition\n",
    "rdd_val = val_df.rdd.zipWithIndex().map(lambda row: (torch.Tensor(row[0]['X_jets']).expand(1, 3, 125, 125), row[1]) ).repartition(80) #(partition_to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "c15f257b-4b7b-487a-9d7d-5c422ce1aa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_val.foreachPartition(save_val_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce8b91-4600-46c6-848d-3ea606d7c5e2",
   "metadata": {},
   "source": [
    "## Lastly, we'll just rename each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d842a1f-0816-47fd-aba4-395ebe322a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list = [tensor_name for tensor_name in os.listdir(train_path) if \"pt\" in tensor_name]\n",
    "tensor_list = sorted([tensor_name for tensor_name in tensor_list], key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "for idx, partition in enumerate(tensor_list):\n",
    "    os.rename(train_path + partition, train_path + \"X_jet_partition_{}.pt\".format(str(idx)))\n",
    "    # print(train_path + partition + \"--->\" + train_path + \"X_jet_partition_{}.pt\".format(str(idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e39ba7-fdb8-47d6-a6b6-74e7c3b3079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list = [tensor_name for tensor_name in os.listdir(val_path) if \"pt\" in tensor_name]\n",
    "tensor_list = sorted([tensor_name for tensor_name in tensor_list], key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "for idx, partition in enumerate(tensor_list):\n",
    "    os.rename(val_path + partition, val_path + \"X_jet_partition_{}.pt\".format(str(idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e3e4f-3fe0-4ff6-9fef-be207d8ed030",
   "metadata": {},
   "source": [
    "### Just some sanity checking, to see the shape of saved tensors and number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "861e4e60-7239-4644-bafb-bee9d9506ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([332, 3, 125, 125])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(train_path + \"X_jet_partition_0.pt\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c2db34c3-e48c-4467-ba9a-8b8e1308cf33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([398, 3, 125, 125])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(val_path + \"X_jet_partition_0.pt\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5514bf2f-7256-41ea-9904-a13c9f7a49d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_train.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "66cfff33-2fc7-4d16-aba8-327491231239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_val.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "df181679-6033-494f-bdf5-1cc297fe8d24",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_jet_partition_0.pt',\n",
       " 'X_jet_partition_10.pt',\n",
       " 'X_jet_partition_20.pt',\n",
       " 'X_jet_partition_30.pt',\n",
       " 'X_jet_partition_40.pt',\n",
       " 'X_jet_partition_50.pt',\n",
       " 'X_jet_partition_60.pt',\n",
       " 'X_jet_partition_70.pt',\n",
       " 'X_jet_partition_80.pt',\n",
       " 'X_jet_partition_90.pt',\n",
       " 'X_jet_partition_100.pt',\n",
       " 'X_jet_partition_110.pt',\n",
       " 'X_jet_partition_120.pt',\n",
       " 'X_jet_partition_130.pt',\n",
       " 'X_jet_partition_140.pt',\n",
       " 'X_jet_partition_150.pt',\n",
       " 'X_jet_partition_160.pt',\n",
       " 'X_jet_partition_170.pt',\n",
       " 'X_jet_partition_180.pt',\n",
       " 'X_jet_partition_190.pt',\n",
       " 'X_jet_partition_200.pt',\n",
       " 'X_jet_partition_210.pt',\n",
       " 'X_jet_partition_220.pt',\n",
       " 'X_jet_partition_230.pt',\n",
       " 'X_jet_partition_240.pt',\n",
       " 'X_jet_partition_250.pt',\n",
       " 'X_jet_partition_260.pt',\n",
       " 'X_jet_partition_270.pt',\n",
       " 'X_jet_partition_280.pt',\n",
       " 'X_jet_partition_290.pt',\n",
       " 'X_jet_partition_300.pt',\n",
       " 'X_jet_partition_310.pt',\n",
       " 'X_jet_partition_320.pt',\n",
       " 'X_jet_partition_330.pt',\n",
       " 'X_jet_partition_340.pt',\n",
       " 'X_jet_partition_350.pt',\n",
       " 'X_jet_partition_360.pt',\n",
       " 'X_jet_partition_370.pt',\n",
       " 'X_jet_partition_380.pt',\n",
       " 'X_jet_partition_390.pt',\n",
       " 'X_jet_partition_400.pt',\n",
       " 'X_jet_partition_410.pt',\n",
       " 'X_jet_partition_420.pt',\n",
       " 'X_jet_partition_430.pt',\n",
       " 'X_jet_partition_440.pt',\n",
       " 'X_jet_partition_450.pt',\n",
       " 'X_jet_partition_460.pt',\n",
       " 'X_jet_partition_470.pt',\n",
       " 'X_jet_partition_480.pt',\n",
       " 'X_jet_partition_490.pt',\n",
       " 'X_jet_partition_500.pt',\n",
       " 'X_jet_partition_510.pt',\n",
       " 'X_jet_partition_520.pt',\n",
       " 'X_jet_partition_530.pt',\n",
       " 'X_jet_partition_540.pt',\n",
       " 'X_jet_partition_550.pt',\n",
       " 'X_jet_partition_560.pt',\n",
       " 'X_jet_partition_570.pt',\n",
       " 'X_jet_partition_580.pt',\n",
       " 'X_jet_partition_590.pt',\n",
       " 'X_jet_partition_600.pt',\n",
       " 'X_jet_partition_610.pt',\n",
       " 'X_jet_partition_620.pt',\n",
       " 'X_jet_partition_630.pt',\n",
       " 'X_jet_partition_640.pt',\n",
       " 'X_jet_partition_650.pt',\n",
       " 'X_jet_partition_660.pt',\n",
       " 'X_jet_partition_670.pt',\n",
       " 'X_jet_partition_680.pt',\n",
       " 'X_jet_partition_690.pt',\n",
       " 'X_jet_partition_700.pt',\n",
       " 'X_jet_partition_710.pt',\n",
       " 'X_jet_partition_720.pt',\n",
       " 'X_jet_partition_730.pt',\n",
       " 'X_jet_partition_740.pt',\n",
       " 'X_jet_partition_750.pt',\n",
       " 'X_jet_partition_760.pt',\n",
       " 'X_jet_partition_770.pt',\n",
       " 'X_jet_partition_780.pt',\n",
       " 'X_jet_partition_790.pt',\n",
       " 'X_jet_partition_800.pt',\n",
       " 'X_jet_partition_810.pt',\n",
       " 'X_jet_partition_820.pt',\n",
       " 'X_jet_partition_830.pt',\n",
       " 'X_jet_partition_839.pt',\n",
       " 'X_jet_partition_849.pt',\n",
       " 'X_jet_partition_859.pt',\n",
       " 'X_jet_partition_869.pt',\n",
       " 'X_jet_partition_879.pt',\n",
       " 'X_jet_partition_889.pt',\n",
       " 'X_jet_partition_899.pt',\n",
       " 'X_jet_partition_909.pt',\n",
       " 'X_jet_partition_919.pt',\n",
       " 'X_jet_partition_929.pt',\n",
       " 'X_jet_partition_939.pt',\n",
       " 'X_jet_partition_949.pt',\n",
       " 'X_jet_partition_959.pt',\n",
       " 'X_jet_partition_969.pt',\n",
       " 'X_jet_partition_979.pt',\n",
       " 'X_jet_partition_989.pt',\n",
       " 'X_jet_partition_999.pt',\n",
       " 'X_jet_partition_1009.pt',\n",
       " 'X_jet_partition_1019.pt',\n",
       " 'X_jet_partition_1029.pt',\n",
       " 'X_jet_partition_1039.pt',\n",
       " 'X_jet_partition_1049.pt',\n",
       " 'X_jet_partition_1059.pt',\n",
       " 'X_jet_partition_1069.pt',\n",
       " 'X_jet_partition_1079.pt',\n",
       " 'X_jet_partition_1089.pt',\n",
       " 'X_jet_partition_1099.pt',\n",
       " 'X_jet_partition_1109.pt',\n",
       " 'X_jet_partition_1119.pt',\n",
       " 'X_jet_partition_1129.pt',\n",
       " 'X_jet_partition_1139.pt',\n",
       " 'X_jet_partition_1149.pt',\n",
       " 'X_jet_partition_1159.pt',\n",
       " 'X_jet_partition_1169.pt',\n",
       " 'X_jet_partition_1179.pt',\n",
       " 'X_jet_partition_1189.pt',\n",
       " 'X_jet_partition_1199.pt',\n",
       " 'X_jet_partition_1209.pt',\n",
       " 'X_jet_partition_1219.pt',\n",
       " 'X_jet_partition_1229.pt',\n",
       " 'X_jet_partition_1239.pt',\n",
       " 'X_jet_partition_1249.pt',\n",
       " 'X_jet_partition_1259.pt',\n",
       " 'X_jet_partition_1269.pt',\n",
       " 'X_jet_partition_1279.pt',\n",
       " 'X_jet_partition_1289.pt',\n",
       " 'X_jet_partition_1299.pt',\n",
       " 'X_jet_partition_1309.pt',\n",
       " 'X_jet_partition_1319.pt',\n",
       " 'X_jet_partition_1329.pt',\n",
       " 'X_jet_partition_1339.pt',\n",
       " 'X_jet_partition_1349.pt',\n",
       " 'X_jet_partition_1359.pt',\n",
       " 'X_jet_partition_1369.pt',\n",
       " 'X_jet_partition_1379.pt',\n",
       " 'X_jet_partition_1389.pt',\n",
       " 'X_jet_partition_1399.pt',\n",
       " 'X_jet_partition_1409.pt',\n",
       " 'X_jet_partition_1419.pt',\n",
       " 'X_jet_partition_1429.pt',\n",
       " 'X_jet_partition_1439.pt',\n",
       " 'X_jet_partition_1449.pt',\n",
       " 'X_jet_partition_1459.pt',\n",
       " 'X_jet_partition_1469.pt',\n",
       " 'X_jet_partition_1479.pt',\n",
       " 'X_jet_partition_1489.pt',\n",
       " 'X_jet_partition_1499.pt',\n",
       " 'X_jet_partition_1509.pt',\n",
       " 'X_jet_partition_1519.pt',\n",
       " 'X_jet_partition_1529.pt',\n",
       " 'X_jet_partition_1539.pt',\n",
       " 'X_jet_partition_1549.pt',\n",
       " 'X_jet_partition_1559.pt',\n",
       " 'X_jet_partition_1569.pt',\n",
       " 'X_jet_partition_1579.pt',\n",
       " 'X_jet_partition_1589.pt',\n",
       " 'X_jet_partition_1599.pt',\n",
       " 'X_jet_partition_1609.pt',\n",
       " 'X_jet_partition_1619.pt',\n",
       " 'X_jet_partition_1629.pt',\n",
       " 'X_jet_partition_1639.pt',\n",
       " 'X_jet_partition_1649.pt',\n",
       " 'X_jet_partition_2221.pt',\n",
       " 'X_jet_partition_2231.pt',\n",
       " 'X_jet_partition_2241.pt',\n",
       " 'X_jet_partition_2251.pt',\n",
       " 'X_jet_partition_2261.pt',\n",
       " 'X_jet_partition_2271.pt',\n",
       " 'X_jet_partition_2281.pt',\n",
       " 'X_jet_partition_2291.pt',\n",
       " 'X_jet_partition_2301.pt',\n",
       " 'X_jet_partition_2311.pt',\n",
       " 'X_jet_partition_2321.pt',\n",
       " 'X_jet_partition_2331.pt',\n",
       " 'X_jet_partition_2341.pt',\n",
       " 'X_jet_partition_2351.pt',\n",
       " 'X_jet_partition_2361.pt',\n",
       " 'X_jet_partition_2371.pt',\n",
       " 'X_jet_partition_2381.pt',\n",
       " 'X_jet_partition_2391.pt',\n",
       " 'X_jet_partition_2401.pt',\n",
       " 'X_jet_partition_2411.pt',\n",
       " 'X_jet_partition_2421.pt',\n",
       " 'X_jet_partition_2431.pt',\n",
       " 'X_jet_partition_2441.pt',\n",
       " 'X_jet_partition_3061.pt',\n",
       " 'X_jet_partition_3071.pt',\n",
       " 'X_jet_partition_3081.pt',\n",
       " 'X_jet_partition_3091.pt',\n",
       " 'X_jet_partition_3101.pt',\n",
       " 'X_jet_partition_3111.pt',\n",
       " 'X_jet_partition_3121.pt',\n",
       " 'X_jet_partition_3131.pt',\n",
       " 'X_jet_partition_3141.pt',\n",
       " 'X_jet_partition_3151.pt',\n",
       " 'X_jet_partition_3161.pt',\n",
       " 'X_jet_partition_3171.pt',\n",
       " 'X_jet_partition_3181.pt',\n",
       " 'X_jet_partition_3191.pt',\n",
       " 'X_jet_partition_3201.pt',\n",
       " 'X_jet_partition_3211.pt',\n",
       " 'X_jet_partition_3221.pt',\n",
       " 'X_jet_partition_3231.pt',\n",
       " 'X_jet_partition_3241.pt',\n",
       " 'X_jet_partition_3251.pt',\n",
       " 'X_jet_partition_3264.pt',\n",
       " 'X_jet_partition_3274.pt',\n",
       " 'X_jet_partition_3284.pt',\n",
       " 'X_jet_partition_3294.pt',\n",
       " 'X_jet_partition_3304.pt',\n",
       " 'X_jet_partition_3314.pt',\n",
       " 'X_jet_partition_3324.pt',\n",
       " 'X_jet_partition_3334.pt',\n",
       " 'X_jet_partition_3344.pt',\n",
       " 'X_jet_partition_3354.pt',\n",
       " 'X_jet_partition_3364.pt',\n",
       " 'X_jet_partition_3374.pt',\n",
       " 'X_jet_partition_3384.pt',\n",
       " 'X_jet_partition_3394.pt',\n",
       " 'X_jet_partition_3404.pt',\n",
       " 'X_jet_partition_3414.pt',\n",
       " 'X_jet_partition_3424.pt',\n",
       " 'X_jet_partition_3434.pt',\n",
       " 'X_jet_partition_3444.pt',\n",
       " 'X_jet_partition_3454.pt',\n",
       " 'X_jet_partition_3464.pt',\n",
       " 'X_jet_partition_3474.pt',\n",
       " 'X_jet_partition_3484.pt',\n",
       " 'X_jet_partition_3494.pt',\n",
       " 'X_jet_partition_3504.pt',\n",
       " 'X_jet_partition_3514.pt',\n",
       " 'X_jet_partition_3524.pt',\n",
       " 'X_jet_partition_3534.pt',\n",
       " 'X_jet_partition_3544.pt',\n",
       " 'X_jet_partition_3554.pt',\n",
       " 'X_jet_partition_3564.pt',\n",
       " 'X_jet_partition_3574.pt',\n",
       " 'X_jet_partition_3584.pt',\n",
       " 'X_jet_partition_3594.pt',\n",
       " 'X_jet_partition_3604.pt',\n",
       " 'X_jet_partition_3614.pt',\n",
       " 'X_jet_partition_3624.pt',\n",
       " 'X_jet_partition_3634.pt',\n",
       " 'X_jet_partition_3644.pt',\n",
       " 'X_jet_partition_3654.pt',\n",
       " 'X_jet_partition_3664.pt',\n",
       " 'X_jet_partition_3674.pt',\n",
       " 'X_jet_partition_3684.pt',\n",
       " 'X_jet_partition_3694.pt',\n",
       " 'X_jet_partition_3704.pt',\n",
       " 'X_jet_partition_3714.pt',\n",
       " 'X_jet_partition_3724.pt',\n",
       " 'X_jet_partition_3734.pt',\n",
       " 'X_jet_partition_3744.pt',\n",
       " 'X_jet_partition_3754.pt',\n",
       " 'X_jet_partition_6178.pt',\n",
       " 'X_jet_partition_6188.pt',\n",
       " 'X_jet_partition_6198.pt',\n",
       " 'X_jet_partition_6208.pt',\n",
       " 'X_jet_partition_6218.pt',\n",
       " 'X_jet_partition_6228.pt',\n",
       " 'X_jet_partition_6238.pt',\n",
       " 'X_jet_partition_6248.pt',\n",
       " 'X_jet_partition_6258.pt',\n",
       " 'X_jet_partition_6268.pt',\n",
       " 'X_jet_partition_6278.pt',\n",
       " 'X_jet_partition_6288.pt',\n",
       " 'X_jet_partition_6298.pt',\n",
       " 'X_jet_partition_6308.pt',\n",
       " 'X_jet_partition_6318.pt',\n",
       " 'X_jet_partition_6328.pt',\n",
       " 'X_jet_partition_6338.pt',\n",
       " 'X_jet_partition_6348.pt',\n",
       " 'X_jet_partition_6358.pt',\n",
       " 'X_jet_partition_6368.pt',\n",
       " 'X_jet_partition_6378.pt',\n",
       " 'X_jet_partition_6388.pt',\n",
       " 'X_jet_partition_6398.pt',\n",
       " 'X_jet_partition_6408.pt',\n",
       " 'X_jet_partition_6418.pt',\n",
       " 'X_jet_partition_6428.pt',\n",
       " 'X_jet_partition_6438.pt',\n",
       " 'X_jet_partition_6448.pt',\n",
       " 'X_jet_partition_6458.pt',\n",
       " 'X_jet_partition_6468.pt',\n",
       " 'X_jet_partition_7831.pt',\n",
       " 'X_jet_partition_7841.pt',\n",
       " 'X_jet_partition_7851.pt',\n",
       " 'X_jet_partition_7861.pt',\n",
       " 'X_jet_partition_7871.pt',\n",
       " 'X_jet_partition_7881.pt',\n",
       " 'X_jet_partition_7891.pt',\n",
       " 'X_jet_partition_7901.pt',\n",
       " 'X_jet_partition_7911.pt',\n",
       " 'X_jet_partition_7921.pt',\n",
       " 'X_jet_partition_7931.pt']"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fb483c3-44d2-4bf2-abdb-02f2662fc1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:13, 21.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 'X_jet_partition_0.pt'),\n",
       " (1, 'X_jet_partition_0.pt'),\n",
       " (2, 'X_jet_partition_0.pt')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Right now, we will create a map of tensor_idx -> partition_num.\n",
    "It's basically a lookup table for faster performance.\n",
    "\n",
    "Since the training data is already shuffled, we'll use a SequentialSampler in Torch,\n",
    "and for each tensor idx, we load each partition only as we need, or keep it in cache.\n",
    "\"\"\"\n",
    "\n",
    "train_tensor_map = {}\n",
    "\n",
    "tensor_list = [tensor_name for tensor_name in os.listdir(train_path) if \"pt\" in tensor_name]\n",
    "tensor_list = sorted([tensor_name for tensor_name in tensor_list], key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "cum_sum = 0\n",
    "for partition_idx, tensor_name in tqdm(enumerate(tensor_list)):\n",
    "\n",
    "    \n",
    "    tensor_num = int(tensor_name.split(\"_\")[-1].split(\".\")[0])\n",
    "    partition_size = torch.load(train_path + tensor_name).shape[0]\n",
    "    \n",
    "    for tensor_idx in range(partition_size):\n",
    "        train_tensor_map[cum_sum + tensor_idx] = tensor_name\n",
    "        \n",
    "    cum_sum += partition_size    \n",
    "list(train_tensor_map.items())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d66093ee-1c67-49b7-ae3c-fc9b23291398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:03, 22.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 'X_jet_partition_0.pt'),\n",
       " (1, 'X_jet_partition_0.pt'),\n",
       " (2, 'X_jet_partition_0.pt')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_tensor_map = {}\n",
    "\n",
    "tensor_list = [tensor_name for tensor_name in os.listdir(val_path) if \"pt\" in tensor_name]\n",
    "tensor_list = sorted([tensor_name for tensor_name in tensor_list], key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "cum_sum = 0\n",
    "for partition_idx, tensor_name in tqdm(enumerate(tensor_list)):\n",
    "\n",
    "    tensor_num = tensor_name.split(\"_\")[-1].split(\".\")[0]\n",
    "    partition_size = torch.load(val_path + tensor_name).shape[0]\n",
    "    \n",
    "    for tensor_idx in range(partition_size):\n",
    "        val_tensor_map[cum_sum + tensor_idx] = tensor_name\n",
    "    cum_sum += partition_size \n",
    "list(val_tensor_map.items())[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aaad8a-641d-4f8b-98f6-933b92710dcf",
   "metadata": {},
   "source": [
    "### We also need to save the labels, but since these are way less memory than tensors, the process is straightforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "039eeb41-9f6f-47df-aff6-f4c1ca7f506a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "y_train = train_df.select(['y']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1ca16d2c-591a-458a-a86d-c74a53848fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "y_val = val_df.select(['y']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "61aff4f8-0f66-46ba-8876-0abb90ab3762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111510"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e0a33b1b-889a-4cbf-baa9-dd860361df07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27796"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a3b35b0a-ae82-4b72-881c-a77695effdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "19636763-f3f2-48df-861c-ee780f888e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to flatten this array\n",
    "y_train = np.array(y_train).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ad0b5619-ebcd-4ed6-8b9b-89959892de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to flatten this array\n",
    "y_val = np.array(y_val).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "03fcca35-ef0f-417f-b42a-fd617c0dbc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"datasets/task_ii/train/y/y.npz\", y= y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7f1362e1-5ac3-46a3-84a5-07d4467a7700",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"datasets/task_ii/validation/y/y.npz\", y= y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6956e98-3e25-45df-a079-6b89149d3fb4",
   "metadata": {},
   "source": [
    "### And define our Quark-Gluon dataset in Torch format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ec23dd3-6582-4184-9355-ed67be187955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a PyTorch Dataset that can load data from a Spark DataFrame\n",
    "class QuarkGluonDataset(Dataset):\n",
    "    def __init__(self, data_path, tensor_map, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data_path = data_path\n",
    "        self.tensor_map = tensor_map\n",
    "        self.partition = tensor_map[0]\n",
    "        self.starting_tensor_idx = 0\n",
    "        self.end_tensor_idx = np.max(list(tensor_map.keys()))\n",
    "        self.cached_tensors = torch.load(data_path + \"X_jets/\" + self.partition)\n",
    "        self.labels = torch.tensor(dict(np.load(data_path + \"y/y.npz\", \"r\"))['y'], dtype=torch.long)\n",
    "        self.flag = 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "\n",
    "        # if the sequential sampler hits the last tensor in the partition, then we reset\n",
    "        # all internal states of the dataset. This is required to reiterate the dataloader\n",
    "        # for more than 1 epoch.\n",
    "        if idx == self.end_tensor_idx:\n",
    "            \n",
    "            del self.partition\n",
    "            \n",
    "            idx -= self.starting_tensor_idx\n",
    "            self.starting_tensor_idx = 0\n",
    "            \n",
    "            self.partition = self.tensor_map[0]\n",
    "            \n",
    "            X = self.cached_tensors[idx - self.starting_tensor_idx]\n",
    "            \n",
    "            del self.cached_tensors\n",
    "            self.cached_tensors = torch.load(self.data_path + \"X_jets/\" + self.partition) \n",
    "            \n",
    "            return X, self.labels[idx]\n",
    "\n",
    "        # if tensor index (idx) advanced to a new partition, only then we update cache.\n",
    "        elif self.tensor_map[idx] != self.partition:\n",
    "\n",
    "            self.starting_tensor_idx += self.cached_tensors.shape[0]\n",
    "            \n",
    "            del self.cached_tensors\n",
    "            del self.partition\n",
    "            \n",
    "            self.partition = self.tensor_map[idx]\n",
    "            self.cached_tensors = torch.load(self.data_path + \"X_jets/\" + self.partition) \n",
    "\n",
    "        X = self.cached_tensors[idx - self.starting_tensor_idx] #torch.load(self.data_path + \"X_jet_\" + str(idx) + \".pt\")\n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "            \n",
    "        return X, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49ef8123-4b6e-4f66-8df6-8e4ae155b2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = QuarkGluonDataset(\"datasets/task_ii/train/\", train_tensor_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22d29a92-a13c-4518-89c0-2f6331179874",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = QuarkGluonDataset(\"datasets/task_ii/validation/\", val_tensor_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "369ca652-bcea-404d-b415-5eb1864fcb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17c2ddc2-9105-4297-9274-87765609d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9c03303-1539-43de-9eae-f60cbd5f0970",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\"train\": train_dataloader,\n",
    "               \"test_dataloader\": val_dataloader}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e736871b-567a-4d1c-b608-8c9e825ef030",
   "metadata": {},
   "source": [
    "## Now, for the 12-layered VGG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf383d2-07c5-471a-a544-1f53a3599cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VGG is a deep convolutional neural network with 16 layers that are trainable, known for its use of small 3x3 filters \n",
    "throughout the network. This is a relatively simple model and at the same time, very good performance on classical image-recognition\n",
    "tasks.\n",
    "\n",
    "In a nutshell, each layer consists of convolution (3,3) -> ReLU -> maxpool.\n",
    "At the end, some fully-connected layers are stacked before the final logits, e.g.:\n",
    "\n",
    "                                            *** VGG-12 architecture ***\n",
    "          ___________________     ____      _______                ______      _____     __________\n",
    "   M -->  |convolution (3,3)| -> |ReLU| -> |maxpool| -> ...... -> |FC (1)| -> |ReLU| -> |FC(logits)| -> Quarks/Gluons\n",
    "          -------------------    ------    ---------              --------    ------    ------------\n",
    "\n",
    "Where M is a tensor of shape (batch_size, 3, 125, 125). \n",
    "\n",
    "Here, we modify the network to have only 12 layers, consisting of 10 convolutions and 2 FC.\n",
    "\n",
    "\"\"\"\n",
    "class VGG12(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16_NET, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv8 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv9 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc14 = nn.Linear(25088, 64)\n",
    "        # self.fc15 = nn.Linear(64, 64)\n",
    "        self.fc15 = nn.Linear(64, 2)\n",
    "        # self.fc16 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.relu(self.conv9(x))\n",
    "        x = F.relu(self.conv10(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = F.relu(self.fc14(x))\n",
    "        x = F.dropout(x, 0.5) # this prevents overfitting\n",
    "        x = F.relu(self.fc15(x))\n",
    "        # x = F.dropout(x, 0.5)\n",
    "        # x = self.fc16(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9abc1e9f-f78c-4136-9e24-a3deea063987",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG12()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "12a9f671-79d0-4d19-940e-e6b310c72726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.0340e-02,  3.0663e-02],\n",
       "        [ 9.0766e-02,  9.8202e-05],\n",
       "        [ 1.0704e-01,  5.9923e-02],\n",
       "        [ 1.2804e-01,  3.3648e-02],\n",
       "        [ 9.4983e-02, -1.6154e-02],\n",
       "        [ 1.1228e-01, -2.0752e-02],\n",
       "        [ 9.5117e-02,  3.1757e-02],\n",
       "        [ 8.2396e-02,  1.1045e-02]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(next(iter(train_dataloader))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490958d7-c6ec-446b-85fc-eb9389795533",
   "metadata": {},
   "source": [
    "### Let's do a simple sanity check to see if the shape of returned tensor are as expected, an also the output of our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "1b264beb-0680-409e-a2e2-c54fb9f42381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 125, 125])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "13656cb4-4f69-4856-9ed8-4c809cb8f075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(next(iter(train_dataloader))[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77ad0d-364c-4d9e-896a-57c844d55ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Indeed, the input tensors come in batches of 64 samples, each being a 3-channel 125 x 125 matrix M.\n",
    "\n",
    "The output, as expected, consists of 64 samples, each a 2d-tensor representing the logits, \n",
    "that'll later represent probabilities of being either quark or gluon.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b40a590-0ec6-417b-ac1b-850e44136028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        dataloaders,\n",
    "        device: str = 'cpu',\n",
    "        num_epoch: int = 50\n",
    "):\n",
    "    # get model\n",
    "    model# = VGG16_NET()\n",
    "\n",
    "    # define loss function\n",
    "    loss_fun = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # define optimiser\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        batch_losses = 0\n",
    "        model.train()\n",
    "        for batch, data in tqdm(enumerate(dataloaders['train'])):\n",
    "            opt.zero_grad()\n",
    "\n",
    "            features = data[0].to(device)\n",
    "            label = data[1].to(device)\n",
    "\n",
    "            y_pred = model(features)\n",
    "\n",
    "            # print(label, y_pred)\n",
    "            loss = loss_fun(y_pred, label)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # track batch losses\n",
    "            batch_losses += loss.item()\n",
    "\n",
    "        avg_batch_loss = batch_losses / (batch + 1)\n",
    "        print(f'Epoch {epoch} average loss: {avg_batch_loss}')\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        batch_losses = 0\n",
    "        for batch, data in tqdm(enumerate(dataloaders['val'])):\n",
    "\n",
    "            features = data[0].to(device)\n",
    "            label = data[1].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(features)\n",
    "                loss = loss_fun(y_pred, label)\n",
    "\n",
    "\n",
    "            # track batch losses\n",
    "            batch_losses += loss.item()\n",
    "\n",
    "        avg_batch_loss = batch_losses / (batch + 1)\n",
    "        print(f'Epoch {epoch} average loss: {avg_batch_loss}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63abf43b-eb28-4224-b8e6-3b66cd486914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275599a-6f02-4520-a9cf-a55d05ba7908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9555it [1:07:18,  2.37it/s]"
     ]
    }
   ],
   "source": [
    "train_model(model, dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005a73b-580f-4905-aa72-8c309eba7d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import horovod.spark\n",
    "import horovod.torch as hvd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Define a PyTorch Dataset that can load data from a Spark DataFrame\n",
    "class QuarkGluonDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data_path = data_path + \"X_jets/\"\n",
    "        self.labels = pd.read_csv(data_path + \"X_jets/labels.npy\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = torch.tensor(self.df.select('X_jets').collect()[idx][0]).float()\n",
    "        image = torch.load(self.data_path + \"X_jet_\" + str(idx) + \"pt\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, self.labels[idx]\n",
    "        \n",
    "\n",
    "# Define transformations for the MNIST images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Create a PyTorch Dataset\n",
    "mnist_dataset = MNISTDataset(mnist_df, transform=transform)\n",
    "\n",
    "# Horovod: initialize library\n",
    "hvd.init()\n",
    "torch.cuda.set_device(hvd.local_rank())\n",
    "\n",
    "# Horovod: adjust learning rate based on number of GPUs\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr * hvd.size())\n",
    "\n",
    "# Horovod: wrap optimizer with DistributedOptimizer\n",
    "optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n",
    "\n",
    "# Horovod: broadcast parameters & optimizer state\n",
    "hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n",
    "\n",
    "# Create a DataLoader for the MNIST dataset\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    mnist_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "train_loader = DataLoader(mnist_dataset, batch_size=64, sampler=train_sampler)\n",
    "\n",
    "# Define a simple neural network model\n",
    "class SimpleNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.linear = torch.nn.Linear(784, 10)  # MNIST images are 28x28\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.linear(x))\n",
    "\n",
    "model = SimpleNN().cuda()\n",
    "\n",
    "# Train the model\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_sampler.set_epoch(epoch)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "\n",
    "# Close the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc60a0-90ac-4734-8f91-51786837c075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e68288-f8ab-4cf3-b477-99c9094a15be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1780b1a-7944-4e6f-8626-c9f3a5812056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920eb7d-7256-4e06-96f5-90478ae35735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8dfd0-4931-4082-a47d-1cf66d93c67e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c089fb-9638-42b8-9180-2de6969bffab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4fbdea-11b4-42af-884c-b1a9bbe316d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bb1c4dad-fedf-49b7-95ed-896b23be97e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cuda:0\n",
      "Total memory in Gb: 11.77\n",
      "Reserved memory in Gb: 0.00\n",
      "Allocated memory in Gb: 0.00\n",
      "Free memory in Gb: 0.00\n"
     ]
    }
   ],
   "source": [
    "def print_device_usage(device):\n",
    "  tot_memory = torch.cuda.get_device_properties(device).total_memory/1024.0**3\n",
    "  reserved_memory = torch.cuda.memory_reserved(device)/1024.0**3\n",
    "  allocated_memory = torch.cuda.memory_allocated(device)/1024.0**3\n",
    "  free_memory = reserved_memory-allocated_memory  # free inside reserved\n",
    "  print('Total memory in Gb: %.2f'%tot_memory)\n",
    "  print('Reserved memory in Gb: %.2f'%reserved_memory)\n",
    "  print('Allocated memory in Gb: %.2f'%allocated_memory)\n",
    "  print('Free memory in Gb: %.2f'%free_memory)\n",
    "\n",
    "\n",
    "# check if a GPU is available. Otherwise run on CPU\n",
    "device = 'cpu'\n",
    "args_cuda = torch.cuda.is_available()\n",
    "if args_cuda: device = \"cuda:0\"\n",
    "print('device : ',device)\n",
    "if args_cuda: print_device_usage(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e396a662-2a61-4107-b59e-d7b2352743d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7af9953e-3215-46c3-b6de-c679a5eb6494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jetConstituentList', 'jetFeatureNames', 'jetImage', 'jetImageECAL', 'jetImageHCAL', 'jets', 'particleFeatureNames']\n"
     ]
    }
   ],
   "source": [
    "# let's open the file\n",
    "data_dir = 'datasets/Data-MLtutorial/JetDataset/'\n",
    "fileIN = data_dir+'jetImage_7_100p_30000_40000.h5'\n",
    "f = h5py.File(fileIN)\n",
    "# and see what it contains\n",
    "print(list(f.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4192b0ca-bd9f-4bd8-b129-a66f18ab0f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"jetImageHCAL\": shape (10000, 100, 100), type \"<f8\">"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['jetImageHCAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58050a6-c1a0-4be9-82c3-b51d10cfb6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb9593d-2a34-4d81-aad6-a450f3c259fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1851bee7-aed0-4af9-b88a-a4e0b5c98a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9adb6466-d7a0-4ad5-88b7-b0838778f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_NET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16_NET, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv8 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv9 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\n",
    "        # self.conv11 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        # self.conv12 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        # self.conv13 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc14 = nn.Linear(25088, 64)\n",
    "        self.fc15 = nn.Linear(64, 64)\n",
    "        self.fc16 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.relu(self.conv9(x))\n",
    "        x = F.relu(self.conv10(x))\n",
    "        x = self.maxpool(x)\n",
    "        # x = F.relu(self.conv11(x))\n",
    "        # x = F.relu(self.conv12(x))\n",
    "        # x = F.relu(self.conv13(x))\n",
    "        # x = self.maxpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = F.relu(self.fc14(x))\n",
    "        x = F.dropout(x, 0.5) #dropout was included to combat overfitting\n",
    "        x = F.relu(self.fc15(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.fc16(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da0393-f7f5-488c-91e3-6b50abf8cce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951015b4-62fb-4d6b-8011-5d7043afb8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "num_epochs = 300\n",
    "batch_size = 16\n",
    "learning_rate = 0.0003\n",
    "\n",
    "model = ResNet(ResidualBlock).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.001, momentum = 0.9)  \n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87d8a55-c7dc-4307-9107-68e967b397ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a08e2d-aae3-4c2a-834c-9ed790225637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e6bdd-d665-45aa-a106-b95071890642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4f078d5-0325-454c-8dbf-f3b42f03628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ee54d24-de6e-49a0-ac6b-70826759ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16_NET()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f776a-7262-42f6-b0d0-2cab3425a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "device = 'cpu'\n",
    "total_step = len(train_loader)\n",
    "p2d = (40,40,40,40) # pad last dim by (1, 1) and 2nd to last by (2, 2)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        images = F.pad(images, p2d, \"constant\", 0)\n",
    "        # print(images.expand(-1,1,-1,-1).shape)\n",
    "        # images =torch.cat((images, torch.randn(64,1,224,224)), axis=1).shape\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del images, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, loss.item()))\n",
    "            \n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "    \n",
    "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f65504-ee76-49c7-9b9d-c1bd28c6fe72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "quantum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
