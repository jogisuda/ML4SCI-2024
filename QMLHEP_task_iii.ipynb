{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92cfe334-28f4-4a0a-9499-bbc56bcf5815",
   "metadata": {},
   "source": [
    "**Task III - comments about Quantum Computing & Quantum Machine Learning**\n",
    "\n",
    "A few years ago, AI was still called by some people \"sci-fi content\". When Rosenblatt conceived the Perceptron, he dreamed of a day where machines would be able to see and understand the human language. Although he was considered too \"futuristic\" for his time, and even though we had the big AI winter coming later, there were still researchers pushing the field forward; now, self-driving cars can \"see\", and Large Language Models (LLMs) can talk to us. For High Energy Physics (HEP), if we go back to 2012, the Higgs Boson, for example, was discovered through machine learning (ML). Since then, intense research has been happening in the area.\n",
    "\n",
    "Similarly, and going back now many decades ago, Feynman proposes to leverage nature's behavior at its tiniest scales as a form of computation. Fast-forward to 1998, three pioneering scientists conceive us the world's first functional quantum computer with a 2-qubit capacity: Isaac Chuang from the Los Alamos National Laboratory (LANL), Neil Gershenfeld from the Massachusetts Institute of Technology (MIT), and Mark Kubinec from the University of California at Berkeley. As of today, researchers and students can perform high-level operations on real quantum hardware at home, and many technologies are already showing promising results, like Xanadu's photonic hardware and IBM's superconducting qubits. \n",
    "\n",
    "Impactful applications have since ranged from drug discovery, with the precise modeling of molecular interactions at quantum levels, to cryptography and more recently, quantum machine learning (QML), our focus here. The idea is to somehow represent the data into the space of quantum states, called a Hilbert space, and perform parameterized unitary operations on it. Just like classical NNs have proven universal approximation capacity, we know that variational circuits have this same property - just in a different way. Instead of linear transformations intercalated with nonlinear activation functions, unitary operators acting on quantum states have the ability to approximate any continuous (and even some non-continuous) function through partial Fourier series. The question now relies on what can be done with QML, and how to design data-efficient models. Much likely, we won't be using variational circuits for deep fake generation, or even in LLMs. This is something that the realm of classical computers do very well. However, physical data might be one potential area. \n",
    "\n",
    "Nowadays, at the LHC, trillions of particles hit the detectors in extremely short time intervals. Just like the Higgs boson, we might find new anomalous events produced inside the accelerator that could certainly help us understand our own universe. But, at this scale, itâ€™s just impossible to rely solely on any kind of manual analysis. Even with large resources of computing, the data produced at CERN is so massive that training even classical ML (and especially QML) models is still a challenge - we need a better way to do it. An elegant solution is to incorporate symmetry-preserving behavior as a potential inductive bias. Since physics is intimately married with the study of symmetries, this effectively reduces the hypothesis space, facilitating optimization, and also, models that have group-preserving properties are not only data-efficient, but robust to shifts in the data distribution due to the group actions too. This is shown, for example, in the classical LorentzNet paper.\n",
    "\n",
    "On the other hand, if we can incorporate symmetry-preserving priors, we can also ask how to learn such symmetries in the first place, if those are unknown beforehand to start with. A very interesting line of work from Roy T. Forestano et al \"Deep Learning Symmetries and Their Lie Groups, Algebras, and Subalgebras from First Principles\" uses classical ML to learn infinitesimal generators through a loss function the forces the model to learn the properties of a Lie algebra from the data: invariance, infinitesimality, orthogonality and closure. They say that their work can be generalized for internal symmetries, which is an interesting view - much is talked about Lorentz-group equivariance, but other symmetries are present in experiments, i.e.: charge should be preserved through track decay chain, color should be confined through hadronic showers, there are baryonic and leptonic numbers, etc. \"Baking\" such symmetries is an interesting angle. \n",
    "\n",
    "Finally, for my background, I finished my master's last year in QML, where I showed that neurodegenerative diseases like Parkinson's can be flagged through the speech using parameterized circuits. I have more than 6 years of Python experience, 3+ years of classical ML background (PyTorch, scikit-learn for development), and 2+ years (my master's) of quantum computing and QML (for coding, my go-to package is Pennylane, but I also have participated in a hackathon using Qiskit). Last year, I had the great opportunity to be invited as a teaching assistant for a short extension course about QML and quantum optimization at the Campinas State University (UNICAMP), here in Brazil, where we could cover a broad range of topics, from the basics of QML, like encoding, to QCNNs, quantum kernels, and even an application of QGANS for High Energy Physics, which was very engaging for the students.\n",
    "\n",
    "Given my motivated comments about QC, QML and symmetry-preserving models above, I would like to work on some project related. I sent a proposal via the GSoC portal for the Quantum Graph Neural Networks for High Energy Physics, where I proposed a Quantum Lie-Equivatiant Neural Network, and some ideas to make it IR(C) safe. The idea of the model is to substitute the neural nets $\\phi_{e}, \\phi_{x}, \\phi_{h}, \\phi_{m}$ for parameterized circuits, or to have a hybrid version. Secondly, given a learned Lie algebra (i.e. using Roy's method, or another one like the LieGAN), we can extract a corresponding metric tensor $J$ by solving:\n",
    "\n",
    "\\begin{equation}\n",
    "L\\cdot J + J\\cdot L^{T} = 0,\n",
    "\\end{equation}\n",
    "\n",
    "which is shown in the LieGAN paper. However, I am an enthusiast of the area, and any potential project related to ML/QML for HEP is good for me. For both fun, and since the GSoC is very competitive, I tackled as many exercises as I could, not only from the QMLHEP, but also the CMS/E2E challenge (the infinitesimal operator was a fun one!)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "quantum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
