![image](https://github.com/jogisuda/ML4SCI-2024/assets/22627105/e0430b68-4ff6-44d5-b388-79070fcb5172)

This repo contains my solutions to the tasks for both QML and CMS/E2E. Although I proposed for the QMLHEP "Quantum Graph Neural Networks for High Energy Physics at the LHC" project, I chose to tackle some classical tasks too, which are the CMS files. This is a description:

* **QMLHEP_task_i.ipynb** - my notebook for task I, where some basic operations on variational circuits are performed using Pennylane.
* **QMLHEP_task_ii.ipynb** - classical Graph Neural Networks (GNN) for quark vs gluon tagging. I chose ParticleNet as a starting point, considering only the pseudorapidity, azimuthal angle and transverse momentum initially. Later, I use a LorentzNet with half the size of parameters, and then these features are converted to 4-momenta, which is better suited to the architecture given the it's Lorentz equivariant. Without any modification in the layers, hidden dimensions, etc, the AUC is $3\\%$ higher, which showcases the power of symmetry-preserving behavior.
* **QMLHEP_task_iii.ipynb** - my sincere comments on quantum computing and quantum machine learning.
* **QMLHEP_task_V.ipynb** - contains a skectch and implementation of a Quantum Lie-Equivariant GNN (my proposal). I started first with the official implementation from LorentzNet (as done in **task II**), and for sanity checking, I test for robustness against boosts. Once the model passes the checks, I implement a hybrid version, where $\phi_{e}$ is a parameterized circuit. The same is tested, and the model still holds equivariance. To show that arbitrary metric **$J$** can be used, I change it from the Minkowski metric to the Euclidean one. In this new setup, Lorentz boosts become symmetry-breaking, while rotations in the **$xy$** plane become symmetry-preserving. This illustrates a situation where one has experimental data, which is often noisy, and might contain new symmetries, and one wants to find such symmetries first, ie: in the LieGAN paper, once a generator is learned through adversarial training, one can extract the metric **$J$** by solving for $L\cdot J + J\cdot L^{T} = 0$, as is shown in the paper.
* **QMLHEP_task_VI.ipynb** - "QuantumFidelityNet" is proposed and implemented for quantum representation learning. I took the traditional InfoNCE loss (which is contrastive), and made some changes so that instead of the cosine, a similarity based on the Fidelity is proposed. In my tests, however, I made some slight modifications due to vanishing gradients. A small $\epsilon$ helped solve the problem. I then show training convergence on a small batch of samples, just for proof-of-concept (PoC).
* **QMLHEP_task_VIII.ipynb** - a classical vision transformer is trained on MNIST using Torch Lightning, and then some ideas are discussed for a variational form.

* **CMS_task_i.ipynb** - photons vs electrons trained on ResNet-15, which was implemented from scratch. Since the task explicitly asked for the ResNet-15, which I only found the implementation details in the [corresponding paper](https://www.hindawi.com/journals/amete/2020/6972826/) "ResNet15: Weather Recognition on Traffic Road with Deep Convolutional Neural Network", I took the time to manually implement it based on the details from the paper.
* **CMS_task_iii_e.ipynb** - a convolutional autoencoder is trained on MNIST, and later two approaches are tried for learning an infinitesimal operator on the latent space - the first is the same as in the oracle preserving latent flow, and the other one from the LieGAN. An extra step extract the metric tensor from the learned algebras. I thought it was very interesting to see that the exponential map is surjective for SO(n), but there are no such guarantees in the latent space (all of this is explained in details in the notebook). To at least alleviate this problem, I tried putting an angle-preserving term in the conv AE loss (which showed convergence, see the notebook), besides the regular mean-squared error (MSE). For this exercise, I was able to found an infinitesimal operator via LieGAN, but not through the oracle preserving latent approach, probably due to the fact that the AE was still producing some bad samples, and for time saving, since my proposal was for QMLHEP, I left out the distillation part - will be fun to revisit this later, however. The results could also suggest that learning a Lie generator through adversarial training could be more robust against noisy samples. As some simple thoughts, I was wondering, in the future, if it's possible to try new symmetry-preserving terms in the loss for the oracle preserving latent flows, or training the GAN with a Wasserstein distance, if possible, for more accurate learned algebras.
* **CMS_task_iii_g.ipynb** - classical Graph Neural Networks (GNN) for quark vs gluon tagging (the same as in QMLHEP task II).
  
* **CMS_task_ii.ipynb** - This was the last task that I did, where dealing with the heavy parquet files was new to me. The full dataset did not fit in memory, so I used PySpark to first dump the files in smaller partitions, and then manually created a DataSet class to map a given tensor to its corresponding partition, which is cached in memory only if necessary. Since I had to focus on my proposal, due to the timeline, I did not have the time to finish this task and train the model. Nonetheless, I left all my implementation there up to the point where I define the VGG-12 architecure.
